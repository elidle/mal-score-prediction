```{r}
library(dplyr)
library(tidyr)
library(tidyverse)
library(ggplot2)
library(gridExtra)
library(kableExtra)
library(GGally)
library(fastDummies)
library(caret)
library(ranger)
library(glmnet)
library(xgboost)
library(broom)
library(doParallel)
library(parallel)
library(plotly)
```

```{r}
anime_data <- read.csv("https://raw.githubusercontent.com/elidle/mal-score-prediction/refs/heads/main/data/mal_anime_2015_2024.csv")
```

```{r}
colnames(anime_data)
```

```{r}
# Multi-hot Encoding: genre, animation
studio_dummies <- anime_data |>
  separate_rows(studios, sep = ", ") |>
  mutate(value = 1) |>
  pivot_wider(
    names_from = studios,
    values_from = value,
    values_fill = 0,
    names_prefix = "studio_"
  ) |>
  select(-c(title_english, type, source, year, rating, score, members, favorites, genres))

genre_dummies <- anime_data |>
  separate_rows(genres, sep = ", ") |>
  mutate(value = 1) %>%
  pivot_wider(
    names_from = genres,
    values_from = value,
    values_fill = 0,
    names_prefix = "genre_"
  ) |>
  select(-c(title_english, type, source, year, rating, score, members, favorites, studios))

# Combine encodings column-wise and
# Remove unused/non-predictor columns (title_english, year, favorites)
anime_data_encoded <- bind_cols(
  anime_data |> select(score, members, type, source, rating),
  studio_dummies,
  genre_dummies
)

# log members
anime_data_encoded$log_members <- log(anime_data_encoded$members)
anime_data_encoded <- anime_data_encoded |> select(-members)

anime_data_encoded$score <- as.numeric(anime_data_encoded$score)
```

```{r}
# Parallel processing for training speedup
# Load required packages

# Create cluster
cl <- makePSOCKcluster(4)

# Register the cluster
registerDoParallel(cl)
```

```{r}
# Create train control for 10-fold CV
set.seed(2108)
train_control <- trainControl(
  method = "cv",
  number = 10,
  savePredictions = "final",
  allowParallel = TRUE
)
```

```{r}
# 1. Multiple Linear Regression
lm_model <- train(
  score ~ .,
  data = anime_data_encoded,
  method = "lm",
  trControl = train_control
)

print(lm_model)
```

```{r}
create_signif_plot <- function(model, title) {
  tidy_coef <- tidy(model$finalModel) |>
  arrange(desc(abs(estimate))) |>
  head(20) |>
  mutate(
    direction = ifelse(estimate > 0, "Positive", "Negative"),
    term = reorder(term, estimate)
  )

  ggplot(tidy_coef, aes(x = term, y = estimate, fill = estimate)) +
  geom_bar(stat = "identity", width = 0.7) +
  scale_fill_gradient2(
    high = "#d7191c",
    mid = "white",
    low = "#2c7bb6",
    midpoint = 0,
    name = "Effect Size"
  ) +
  coord_flip() +
  labs(
    title = title,
    x = NULL,
    y = "Coefficient Estimate"
  ) +
  theme_minimal() +
  theme(legend.position = "right")
}
```

```{r}
create_signif_plot(lm_model, "Estimates of Top 20 Most Significant Predictors (MLR)")
```

```{r}
# Notice that the most significant predictors are all either the intercept or
# studios with few associated entries, which might concern us with overfitting

# Filter the dataset to only keep encoded values with at least 12 entries
# associated with it, then refit MLR for analysis
nzv <- nearZeroVar(anime_data_encoded, freqCut = 200)
anime_data_filtered <- anime_data_encoded[, -nzv]

lm_model_filtered <- train(
  score ~ .,
  data = anime_data_filtered,
  method = "lm",
  trControl = train_control
)

print(lm_model_filtered)
```

```{r}
create_signif_plot(lm_model_filtered, "Estimates of Top 20 Most Significant Predictors (MLR, filtered)")
```

```{r}
# From the plot we still have several studios as significant predictors, which might
# imply that it is justified to keep them in our dataset as studios does
# have a relationship with MAL scores. We will conduct further analaysis using
# ridge regression, which punishes complex models using regularization while
# maintaining linear assumption.
```

```{r}
# 2. Regularized Regression (ElasticNet)
glmnet_model <- train(
  score ~ .,
  data = anime_data_encoded,
  method = "glmnet",
  tuneGrid = expand.grid(
    alpha = seq(0, 1, 0.1), # 0 = ridge, 1 = lasso
    lambda = 10^seq(-3, -1, length = 100)
  ),
  trControl = train_control,
  standardize = TRUE
)
```

```{r}
glmnet_model$results |>
  filter(alpha == glmnet_model$bestTune$alpha,
         lambda == glmnet_model$bestTune$lambda)
```

```{r}
create_imp_plot <- function(model, low, high, title) {
  imp <- varImp(model, scale = FALSE)
  imp_data <- imp$importance |>
  tibble::rownames_to_column("Variable") |>
  arrange(desc(Overall)) |>
  head(20)
  
  # p <- ggplot(imp_data, aes(x = reorder(Variable, Overall), y = Overall, 
  #                         text = paste("Variable:", Variable, "<br>Importance:", Overall))) +
  # geom_bar(stat = "identity", fill = "#3182bd") +
  # coord_flip() +
  # labs(x = NULL, y = "Importance Score") +
  # theme_minimal()
  
  p <- ggplot(imp_data, aes(x = reorder(Variable, Overall), y = Overall, 
                            fill = Overall, 
                            text = paste("Variable:", Variable, "<br>Importance:", Overall))) +
  geom_bar(stat = "identity", width = 0.7) +
  scale_fill_gradient(
    low = "#f0f9e8",
    high = "#238b45",
    name = "Importance"
  ) +
  coord_flip() +
  labs(
    title = title,
    x = NULL,
    y = "Importance Score"
  ) +
  theme_minimal(base_size = 10) +
  theme(
    plot.title = element_text(face = "bold", size = 13),
    axis.text.y = element_text(size = 10),
    legend.position = "right",
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank()
  )

  ggplotly(p, tooltip = "text")
}
```

```{r}
create_imp_plot(glmnet_model, "#f0f9e8", "#238b45", "Top 20 Most Important Variables (ElasticNet)")
```

```{r}
# Above plot shows predictors that survive regularization. Since most of them
# are studios, then this might justify us to keep including them when building
# subsequent models. Next, we will try two tree-based non-linear models:
# Random Forest and XGBoost.
# The variable importance generated by these models should be robust in
# overfitting and give us more insight about truly significant variables.
```

```{r}
# 3. Random Forest
rf_model <- train(
  score ~ .,
  data = anime_data_encoded,
  method = "ranger",
    tuneGrid = expand.grid(
    mtry = c(5, 10, 20, 50, 100),
    splitrule = "variance",
    min.node.size = c(1, 3, 5)
  ),
  trControl = train_control,
  num.trees = 500,
  importance = "permutation",
  metric = "RMSE"
)
```

```{r}
rf_model$results |>
  filter(mtry == rf_model$bestTune$mtry,
         min.node.size == rf_model$bestTune$min.node.size)
```

```{r}
create_imp_plot(rf_model, "#f0f9e8", "#238b45", "Top 20 Most Important Variables (Random Forest)")
```

```{r}
# 4. XGBoost
xgb_model <- train(
  score ~ .,
  data = anime_data_encoded,
  method = "xgbTree",
  tuneGrid = xgb_grid <- expand.grid(
    nrounds = c(100, 200),
    max_depth = c(3, 6, 9),
    eta = c(0.01, 0.1, 0.3),
    gamma = 0,
    colsample_bytree = c(0.6, 0.8),
    min_child_weight = c(1, 3),
    subsample = c(0.8, 1)
  ),
  trControl = train_control,
  metric = "RMSE",
  nthread = 1
)
```

```{r}
xgb_model$results |>
  filter(nrounds == xgb_model$bestTune$nrounds,
         max_depth == xgb_model$bestTune$max_depth,
         eta == xgb_model$bestTune$eta,
         gamma == xgb_model$bestTune$gamma,
         colsample_bytree == xgb_model$bestTune$colsample_bytree,
         min_child_weight == xgb_model$bestTune$min_child_weight,
         subsample == xgb_model$bestTune$subsample)
```

```{r}
create_imp_plot(xgb_model, "#f0f9e8", "#238b45", "Top 20 Most Important Variables (XGBoost)")
```

```{r}
# Create Actual vs Predicted plot
predictions <- predict(rf_model, newdata = anime_data_encoded)
actual <- anime_data$score
results <- data.frame(Actual = actual, Predicted = predictions)

p <- ggplot(results, aes(x = Actual, y = Predicted, text = paste("Actual: ", round(Actual, 2), "<br>Predicted: ", round(Predicted, 2)))) +
  geom_point(alpha = 0.25, color = "steelblue") +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  labs(title = "Predicted vs Actual MAL Scores (Random Forest)",
       x = "Actual Score",
       y = "Predicted Score") +
  theme_minimal() +
  coord_equal()

# ggplotly(p, tooltip = "text") # Interactive, Laggy
p
```

```{r}
# Close clusters after doing computations
stopCluster(cl)
registerDoSEQ()
```