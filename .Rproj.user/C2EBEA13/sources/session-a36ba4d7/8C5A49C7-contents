---
title: "Model Analysis"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: flatly
    css: style.css
---

## Model Performance Comparison

All models were trained using 10-fold cross validation in order to make efficient use of the relatively small dataset. Below are the performance metrics for each trained model and the hyperpamateres to be tuned using grid search.

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
source("source/model.R")
```

```{r, echo=FALSE, warning=FALSE, style="display: flex;"}
library(knitr)
library(kableExtra)

model_perf <- data.frame(
  Model = c(
    "MLR",
    "MLR (Filtered)",
    "Regularized Regression",
    "Random Forest",
    "XGBoost"
  ),
  RMSE = c(0.59815, 0.60834, 0.5832, 0.5619, 0.55792),
  R2 = c(0.5464, 0.52662, 0.56461, 0.60261, 0.6025),
  MAE = c(0.46136, 0.46969, 0.44931, 0.41889, 0.42065)
)

model_perf |>
  kbl(
    col.names = c("Model", "RMSE", "$R^2$", "MAE"),
    align = "lccc",
    escape = FALSE,
    booktabs = TRUE
  ) |>
  kable_styling(full_width = FALSE, position = "left") |>
  column_spec(2:4, width = "2cm")
```

```{r, echo=FALSE, warning=FALSE, style="display: flex;"}
hyperparams <- data.frame(
  Model = c(
    "Regularized Regression", "Regularized Regression",
    "Random Forest", "Random Forest",
    "XGBoost", "XGBoost", "XGBoost", "XGBoost", "XGBoost", "XGBoost"
  ),
  Hyperparameter = c(
    "alpha", "lambda",
    "mtry", "min.node.size",
    "nrounds", "max_depth", "eta", "colsample_bytree", "min_child_weight", "subsample"
  ),
  `Values Considered` = c(
    "0, 0.1, ..., 1", "10⁻³ to 10⁻¹ (100 values)",
    "5, 10, 20, 50, 100", "1, 3, 5",
    "100, 200", "3, 6, 9", "0.01, 0.1, 0.3", "0.6, 0.8", "1, 3", "0.8, 1"
  ),
  `Final Value` = c(
    "0.5", "0.012",
    "100", "1",
    "200", "9", "0.1", "0.6", "1", "1"
  )
)

hyperparams |>
  kbl(
    col.names = c("Model", "Hyperparameter", "Values Considered", "Final Value"),
    align = "lllc",
    escape = FALSE,
    booktabs = TRUE
  ) |>
  kable_styling(full_width = FALSE, position = "left") |>
  collapse_rows(columns = 1, valign = "top")  # Group rows by model
```

## Model Descriptions {.tabset}

### Multiple Linear Regression

This model assumes a direct linear relationship between predictors and scores and thus is used as an easily interpretable baseline model. As the dataset mostly consists of categorical variables (members being the only numerical variable), the model is directly fit to the dataset without any further variable transformation to provide baseline metrics.

Initial fitting using was done using all predictors. The most significant predictors were either the intercept or studios with few associated entries, which concerns overfitting. The model was then retrained on filtered dataset including only encoded values with ≥12 associated entries. The refit model still identified several studios as significant predictors, which suggests studio information may genuinely influence MAL scores.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
create_signif_plot(lm_model, "Estimates of Top 20 Most Significant Predictors (MLR)")
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
create_signif_plot(lm_model_filtered, "Estimates of Top 20 Most Significant Predictors (MLR, filtered)")
```

### Regularized Regression (ElasticNet)

ElasticNet is a linear regression model that combines L1 (Lasso) and L2 (Ridge) regularization to avoid overfitting and improve prediction accuracy and interpretability. Lasso regularization helps in feature selection by shrinking some coefficients to zero, while Ridge regularization reduces overfitting by penalizing large coefficients. By blending both penalties, ElasticNet is particularly useful when dealing with datasets that have multicollinearity or when the number of predictors is large relative to the number of observations. The hyperparameters to be tuned are `alpha` (the balance between L1 and L2 penalties, with 0 being pure ridge and 1 being pure Lasso) and `lambda` (overall strength of regularization).

This model is used to maintain linear assumptions while penalizing complexity in our analysis. The variable importance plot showed studios remained among the most influential predictors after regularization, further justifying including them to our models.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
create_imp_plot(glmnet_model, "#f0f9e8", "#238b45", "Top 20 Most Important Variables (ElasticNet)")
```

### Random Forest

This model is an ensemble learning method that constructs multiple decision trees during training and averages their predictions to reduce variance and improve accuracy. This approach makes the model robust against overfitting while capturing complex, non-linear relationships in the data like regular Regression Trees. To train the model, the `ranger` package was used instead of the `randomForest` library for better computational efficiency and support for modern tuning parameters. The hyperparameters to be tuned are `mtry` (number of randomly selected features considered at each split) and `min.node.size` (minimum observations in terminal node).

```{r, echo=FALSE, warning=FALSE, message=FALSE}
create_imp_plot(rf_model, "#f0f9e8", "#238b45", "Top 20 Most Important Variables (Random Forest)")
```

### XGBoost

XGBoost is a scalable end-to-end tree boosting system. It is a highly-optimized framework that combines gradient boosting (constructing trees sequentially with each tree correcting the errors of the previous one, minimizing RMSE using gradient descent), regularization, and hardware optimizations. The `xgboost` library is used to train the model. The hyperparameters tuned are `nrounds` (how many sequential trees or boosting iterations is built), `eta` (learning rate), `max_depth` (maximum tree depth), `colsample_bytree` (proportion of features considered at each split), `min_child_weight` (minimum weight needed in child nodes), and `subsample` (fraction of samples per tree).

```{r, echo=FALSE, warning=FALSE, message=FALSE}
create_imp_plot(xgb_model, "#f0f9e8", "#238b45", "Top 20 Most Important Variables (XGBoost)")
```
